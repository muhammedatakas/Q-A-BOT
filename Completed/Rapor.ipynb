{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Q&A System Based on Web Traffic Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "his report presents the development process of an AI-powered question-answering (Q&A) system based on web traffic logs. The system leverages state-of-the-art models for retrieval-augmented generation (RAG) and aims to provide insights from web server access logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "The primary goal of this project is to explore and implement a RAG-based system to answer specific questions derived from web traffic logs. The focus is on experimenting with different models and techniques to optimize performance and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "Initially, the challenge was to find a suitable dataset. While some datasets were available, they were either too large or lacked meaningful content. I considered generating synthetic data using the Faker module but found that it did not reflect real-world scenarios accurately. Eventually, I selected a web server access log dataset from Kaggle, available [here](https://www.kaggle.com/datasets/eliasdabbas/web-server-access-logs), which provided a more realistic foundation for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "The raw log file was converted to a .csv format to facilitate easier manipulation and analysis. Given the size of the data, I opted to take a sample of it for quicker processing. The preprocessing involved using the re module and pandas for parsing and cleaning the data. I then loaded the data using CSVLoader from LangChain, enabling seamless integration into the subsequent processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Embeddings \n",
    "For text embeddings, I utilized the sentence-transformers/all-MiniLM-L6-v2 model from Hugging Face, which offers a balance between performance and computational efficiency. The data was split into manageable chunks using CharacterTextSplitter with a specified chunk size. This approach ensured that the text was adequately prepared for vector storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###VectorStore\n",
    "During the development process, I experimented with different versions of the FAISS vector store, including both CPU and GPU implementations. Initially, I followed documentation that suggested using FAISS directly with custom parameters. However, I found this approach to be slow, particularly in earlier attempts with other notebooks. Consequently, I decided to use LangChain's FAISS.from_documents method, which streamlined the process of storing vectors. FAISS was chosen for its ability to efficiently handle large datasets, making it suitable for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Model\n",
    "The model selection process was iterative and resource-intensive. Due to disk space limitations, I experimented with various models, including T5, GPT-2, and LLaMA3. Although the T5 model produced good results, its token limit (512 tokens) posed a constraint. Therefore, I selected LLaMA3 for the final implementation, as it can process more tokens and exhibits higher intelligence. Additionally, I included a version of the notebook using the T5 model for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Evaluation\n",
    "The system's performance was evaluated based on speed and accuracy. While the LLaMA3 model provided more intelligent answers, it was noticeably slower than T5, which performed faster but was limited by its token capacity. The vector storage process, which is integral to the system's functioning, was found to be time-consuming, affecting the overall efficiency of the system when handling quick queries.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
