{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement re (from versions: none)\n",
      "ERROR: No matching distribution found for re\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Web Traffic Log-Based Q&A System\n",
    "# \n",
    "# This project uses web traffic logs to create a question-answering system powered by a Retrieval-Augmented Generation (RAG) model. The system processes logs, stores them in a vector database, and uses two different language models (LLaMA 3 and Google T5) to generate answers to user queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\"\"\"\n",
    "Import necessary libraries and modules for the Q&A BOT notebook.\n",
    "This code imports the following libraries and modules:\n",
    "- re: Regular expression operations.\n",
    "- pandas: Data manipulation and analysis.\n",
    "- numpy: Numerical computing.\n",
    "- datetime: Date and time manipulation.\n",
    "- faiss: Efficient similarity search and clustering of dense vectors.\n",
    "- tqdm: Progress bar for loops and tasks.\n",
    "- langchain.vectorstores: Vector stores for language embeddings.\n",
    "- langchain_huggingface: Hugging Face embeddings for language models.\n",
    "- langchain.docstore: Document store for storing and retrieving documents.\n",
    "- langchain.schema: Schema for defining document structure.\n",
    "- transformers: State-of-the-art natural language processing models.\n",
    "- langchain_huggingface: Hugging Face pipeline for language models.\n",
    "- langchain.chains: Retrieval-based question answering model.\n",
    "The code also checks for GPU availability, clears the GPU cache, and sets the device to either \"cuda\" or \"cpu\" based on availability.\n",
    "Note: The code assumes that the necessary libraries and modules are already installed.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import faiss\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.schema import Document\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Check for GPU availability\n",
    "import torch\n",
    "# Clear GPU cache before and after running the model\n",
    "torch.cuda.empty_cache()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log parsing and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents: [Document(metadata={'source': 'downsampled_logs.csv', 'row': 0}, page_content='IP: 188.158.164.229\\nIdentity: -\\nUser: -\\nTimestamp: 26/Jan/2019:20:01:52 +0330\\nRequest: GET /image/57274/productModel/150x150 HTTP/1.1\\nStatus: 200\\nSize: 5687\\nReferer: https://www.zanbil.ir/browse/cooktop/%D8%A7%D8%AC%D8%A7%D9%82-%DA%AF%D8%A7%D8%B2-%D8%B5%D9%81%D8%AD%D9%87-%D8%A7%DB%8C\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 1}, page_content='IP: 83.121.146.171\\nIdentity: -\\nUser: -\\nTimestamp: 25/Jan/2019:01:37:21 +0330\\nRequest: GET /image/58606/productModel/200x200 HTTP/1.1\\nStatus: 200\\nSize: 2492\\nReferer: https://www.zanbil.ir/m/filter/b2%2Cp3?page=1\\nUser-Agent: Mozilla/5.0 (Linux; Android 8.1.0; SM-G610F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.99 Mobile Safari/537.36'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 2}, page_content='IP: 178.131.189.208\\nIdentity: -\\nUser: -\\nTimestamp: 25/Jan/2019:19:22:25 +0330\\nRequest: GET /image/31385/productTypeMenu HTTP/1.1\\nStatus: 200\\nSize: 11\\nReferer: https://www.zanbil.ir/product/33598/%D8%AA%D9%84%D9%88%DB%8C%D8%B2%DB%8C%D9%88%D9%86-%D8%A7%D9%84-%D8%A7%DB%8C-%D8%AF%DB%8C-%D8%B3%D8%A7%D9%85%D8%B3%D9%88%D9%86%DA%AF-%D9%85%D8%AF%D9%84-50NU7900-Ultra-HD-4K\\nUser-Agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 3}, page_content='IP: 82.99.202.122\\nIdentity: -\\nUser: -\\nTimestamp: 26/Jan/2019:10:54:28 +0330\\nRequest: GET /image/2/productTypeMenu HTTP/1.1\\nStatus: 200\\nSize: 11\\nReferer: https://www.zanbil.ir/search/%D9%84%D8%A8%D8%A7%D8%B3%D8%B4%D9%88%D9%8A%D9%8A/p0%2Cb2\\nUser-Agent: Mozilla/5.0 (Windows NT 5.1; rv:47.0) Gecko/20100101 Firefox/47.0'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 4}, page_content='IP: 5.209.139.245\\nIdentity: -\\nUser: -\\nTimestamp: 25/Jan/2019:19:32:14 +0330\\nRequest: GET /image/62050/productModel/150x150 HTTP/1.1\\nStatus: 200\\nSize: 2221\\nReferer: https://www.zanbil.ir/m/product/31598/%DB%8C%D8%AE%DA%86%D8%A7%D9%84-%D9%81%D8%B1%DB%8C%D8%B2%D8%B1-%D9%81%D8%B1%DB%8C%D8%B2%D8%B1-%D8%A8%D8%A7%D9%84%D8%A7-%D9%85%DB%8C%D8%AF%DB%8C%D8%A7-%D9%85%D8%AF%D9%84-HQ-585FWE\\nUser-Agent: Mozilla/5.0 (Linux; Android 7.0; WAS-LX1A Build/HUAWEIWAS-LX1A) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.84 Mobile Safari/537.36')]\n",
      "DataFrame head:                 ip identity user                    datetime method  \\\n",
      "0  188.158.164.229        -    -  26/Jan/2019:20:01:52 +0330    GET   \n",
      "1   83.121.146.171        -    -  25/Jan/2019:01:37:21 +0330    GET   \n",
      "2  178.131.189.208        -    -  25/Jan/2019:19:22:25 +0330    GET   \n",
      "3    82.99.202.122        -    -  26/Jan/2019:10:54:28 +0330    GET   \n",
      "4    5.209.139.245        -    -  25/Jan/2019:19:32:14 +0330    GET   \n",
      "\n",
      "                                 url status  size  \\\n",
      "0  /image/57274/productModel/150x150    200  5687   \n",
      "1  /image/58606/productModel/200x200    200  2492   \n",
      "2       /image/31385/productTypeMenu    200    11   \n",
      "3           /image/2/productTypeMenu    200    11   \n",
      "4  /image/62050/productModel/150x150    200  2221   \n",
      "\n",
      "                                             referer  \\\n",
      "0  https://www.zanbil.ir/browse/cooktop/%D8%A7%D8...   \n",
      "1      https://www.zanbil.ir/m/filter/b2%2Cp3?page=1   \n",
      "2  https://www.zanbil.ir/product/33598/%D8%AA%D9%...   \n",
      "3  https://www.zanbil.ir/search/%D9%84%D8%A8%D8%A...   \n",
      "4  https://www.zanbil.ir/m/product/31598/%DB%8C%D...   \n",
      "\n",
      "                                          user_agent  \n",
      "0  Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...  \n",
      "1  Mozilla/5.0 (Linux; Android 8.1.0; SM-G610F) A...  \n",
      "2  Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...  \n",
      "3  Mozilla/5.0 (Windows NT 5.1; rv:47.0) Gecko/20...  \n",
      "4  Mozilla/5.0 (Linux; Android 7.0; WAS-LX1A Buil...  \n",
      "                ip identity user                  datetime method  \\\n",
      "0  188.158.164.229        -    - 2019-01-26 20:01:52+03:30    GET   \n",
      "1   83.121.146.171        -    - 2019-01-25 01:37:21+03:30    GET   \n",
      "2  178.131.189.208        -    - 2019-01-25 19:22:25+03:30    GET   \n",
      "3    82.99.202.122        -    - 2019-01-26 10:54:28+03:30    GET   \n",
      "4    5.209.139.245        -    - 2019-01-25 19:32:14+03:30    GET   \n",
      "\n",
      "                                 url  status  size  \\\n",
      "0  /image/57274/productModel/150x150     200  5687   \n",
      "1  /image/58606/productModel/200x200     200  2492   \n",
      "2       /image/31385/productTypeMenu     200    11   \n",
      "3           /image/2/productTypeMenu     200    11   \n",
      "4  /image/62050/productModel/150x150     200  2221   \n",
      "\n",
      "                                             referer  \\\n",
      "0  https://www.zanbil.ir/browse/cooktop/%D8%A7%D8...   \n",
      "1      https://www.zanbil.ir/m/filter/b2%2Cp3?page=1   \n",
      "2  https://www.zanbil.ir/product/33598/%D8%AA%D9%...   \n",
      "3  https://www.zanbil.ir/search/%D9%84%D8%A8%D8%A...   \n",
      "4  https://www.zanbil.ir/m/product/31598/%DB%8C%D...   \n",
      "\n",
      "                                          user_agent  hour  day  month  year  \\\n",
      "0  Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...    20   26      1  2019   \n",
      "1  Mozilla/5.0 (Linux; Android 8.1.0; SM-G610F) A...     1   25      1  2019   \n",
      "2  Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...    19   25      1  2019   \n",
      "3  Mozilla/5.0 (Windows NT 5.1; rv:47.0) Gecko/20...    10   26      1  2019   \n",
      "4  Mozilla/5.0 (Linux; Android 7.0; WAS-LX1A Buil...    19   25      1  2019   \n",
      "\n",
      "   weekday  status_category                                               text  \n",
      "0        5                2  GET /image/57274/productModel/150x150 (Status:...  \n",
      "1        4                2  GET /image/58606/productModel/200x200 (Status:...  \n",
      "2        4                2  GET /image/31385/productTypeMenu (Status: 200,...  \n",
      "3        5                2  GET /image/2/productTypeMenu (Status: 200, Siz...  \n",
      "4        4                2  GET /image/62050/productModel/150x150 (Status:...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import CSVLoader\n",
    "import re\n",
    "# The following code handles the loading, parsing, and preprocessing of web traffic logs. The logs are parsed to extract useful fields, and the data is then cleaned and structured for further use.\n",
    "\n",
    "# Function to parse the page content\n",
    "def parse_page_content(page_content):\n",
    "    \"\"\"\n",
    "    Parses the content of a web traffic log entry.\n",
    "    \n",
    "    Args:\n",
    "        page_content (str): A single log entry as a string.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing parsed fields like IP, identity, user, datetime, method, etc.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r'IP: (?P<ip>[\\d\\.]+)\\n'\n",
    "        r'Identity: (?P<identity>.+)\\n'\n",
    "        r'User: (?P<user>.+)\\n'\n",
    "        r'Timestamp: (?P<datetime>.+)\\n'\n",
    "        r'Request: (?P<method>\\w+) (?P<url>.+) HTTP/\\d\\.\\d\\n'\n",
    "        r'Status: (?P<status>\\d+)\\n'\n",
    "        r'Size: (?P<size>\\d+)\\n'\n",
    "        r'Referer: (?P<referer>.+)\\n'\n",
    "        r'User-Agent: (?P<user_agent>.+)'\n",
    "    )\n",
    "    match = pattern.search(page_content)\n",
    "    if match:\n",
    "        return match.groupdict()\n",
    "    return {}\n",
    "\n",
    "# Function to downsample the CSV file\n",
    "def downsample_csv(csv_file_path, sample_size, output_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    if sample_size and sample_size < len(df):\n",
    "        df = df.sample(n=sample_size)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    return output_file_path\n",
    "\n",
    "# Load and preprocess logs using CSVLoader\n",
    "def load_and_preprocess_logs(csv_file_path, sample_size=10000):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses web traffic logs from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path (str): Path to the CSV file containing log data.\n",
    "        sample_size (int): Number of samples to load from the file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing processed log data with additional fields for analysis.\n",
    "    \"\"\"\n",
    "    # Downsample the CSV file\n",
    "    downsampled_csv = downsample_csv(csv_file_path, sample_size, 'downsampled_logs.csv')\n",
    "    \n",
    "    # Load the downsampled CSV file\n",
    "    loader = CSVLoader(file_path=downsampled_csv)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Debug: Print the first few documents to check if they are loaded correctly\n",
    "    print(\"Loaded documents:\", documents[:5])\n",
    "    \n",
    "    # Convert documents to DataFrame\n",
    "    data = []\n",
    "    for doc in documents:\n",
    "        parsed_data = parse_page_content(doc.page_content)\n",
    "        data.append(parsed_data)\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Debug: Print the DataFrame to check if it is populated correctly\n",
    "    print(\"DataFrame head:\", df.head())\n",
    "    \n",
    "    # Preprocess logs\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format='%d/%b/%Y:%H:%M:%S %z', errors='coerce')\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['weekday'] = df['datetime'].dt.weekday\n",
    "    df['status'] = df['status'].astype(int, errors='ignore')\n",
    "    df['size'] = df['size'].astype(int, errors='ignore')\n",
    "    df['status_category'] = df['status'] // 100\n",
    "    \n",
    "    # Create a text field for embedding\n",
    "    df['text'] = df.apply(lambda row: f\"{row['method']} {row['url']} (Status: {row['status']}, Size: {row['size']}, IP: {row['ip']})\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "processed_logs = load_and_preprocess_logs('processed_logs.csv', sample_size=10000)\n",
    "\n",
    "print(processed_logs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Anaconda\\envs\\RAG-SYSTEM\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "d:\\AI\\Anaconda\\envs\\RAG-SYSTEM\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# The following section creates a vector database using FAISS and stores the processed log data for efficient retrieval during query processing.\n",
    "\n",
    "\"\"\"\n",
    "Initialize the embedding model using HuggingFaceEmbeddings.\n",
    "Parameters:\n",
    "- model_name (str): The name of the Hugging Face model to use for embeddings.\n",
    "Returns:\n",
    "- embeddings (HuggingFaceEmbeddings): The initialized HuggingFaceEmbeddings object.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Create documents for vector store.\n",
    "Parameters:\n",
    "- processed_logs (DataFrame): The processed logs containing 'text', 'datetime', and 'user_agent' columns.\n",
    "Returns:\n",
    "- documents (list): The list of documents for the vector store, with additional information appended.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Split documents into smaller chunks.\n",
    "Parameters:\n",
    "- chunk_size (int): The size of each chunk.\n",
    "- chunk_overlap (int): The overlap between consecutive chunks.\n",
    "Returns:\n",
    "- texts (list): The list of split documents.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Create the vector store using FAISS.\n",
    "Parameters:\n",
    "- texts (list): The list of split documents.\n",
    "- embeddings (HuggingFaceEmbeddings): The initialized HuggingFaceEmbeddings object.\n",
    "Returns:\n",
    "- vectorstore (FAISS): The created FAISS vector store.\n",
    "\"\"\"\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create documents for vector store\n",
    "documents = [\n",
    "    f\"{row['text']} (Datetime: {row['datetime']}, User Agent: {row['user_agent']})\"\n",
    "    for _, row in processed_logs.iterrows()\n",
    "]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.create_documents(documents)\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rag Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# The RAG (Retrieval-Augmented Generation) model combines retrieval from the vector store and language generation using the LLaMA 3 model. It is set up here to handle user queries.\n",
    "\n",
    "\n",
    "\"\"\"\"\"\n",
    "This code initializes a retrieval-based question answering (QA) system for analyzing web traffic log entries. The system uses the OllamaLLM language model and the RetrievalQA chain from the langchain library.\n",
    "The main steps in the code are as follows:\n",
    "1. Set up logging for the system.\n",
    "2. Initialize the OllamaLLM language model with specific parameters.\n",
    "3. Define a template for generating prompts for the QA system.\n",
    "4. Create a PromptTemplate object with the defined template and input variables.\n",
    "5. Create a RetrievalQA chain using the OllamaLLM model, a retriever, and the PromptTemplate.\n",
    "6. The RetrievalQA chain is ready to be used for answering questions based on the provided log entries.\n",
    ".\"\"\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize OllamaLLM\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3\",  # or the specific model you're using\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "template = \"\"\"\"\n",
    "    You are an expert cybersecurity analyst specializing in web traffic log analysis. You have access to detailed log entries from a high-traffic website. Your task is to provide an in-depth analysis of these logs to answer specific questions posed by users.\n",
    "\n",
    "    When analyzing the logs, follow these guidelines:\n",
    "    1. **Data Integrity:** Ensure the accuracy and completeness of the data by cross-referencing multiple log entries where applicable.\n",
    "    2. **Pattern Recognition:** Identify and explain any significant patterns or anomalies in user behavior, such as:\n",
    "       - Repeated access from specific IP addresses.\n",
    "       - Unusual patterns in user-agent strings (indicating bots, crawlers, or potential attackers).\n",
    "       - Consistent access to specific pages or endpoints at unusual times.\n",
    "    3. **Contextual Correlation:** Correlate log entries across different dimensions (e.g., IP, time, URL) to build a coherent narrative of the events.\n",
    "    4. **Security Implications:** Assess and highlight any potential security concerns, such as:\n",
    "       - Signs of DDoS attacks.\n",
    "       - Unusual traffic spikes that could indicate brute force attempts.\n",
    "       - Access patterns that suggest vulnerability scanning.\n",
    "    5. **Detailed Justification:** For each observation or conclusion, provide specific log entries as evidence. Explain how these logs lead to your conclusions.\n",
    "\n",
    "    If the information requested is not available in the logs or if you are unable to determine an answer, clearly state that the data is inconclusive.\n",
    "\n",
    "    Below are the relevant log entries:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Based on the above logs, answer the following question with detailed reasoning and examples:\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# The following function allows users to input questions about the web traffic logs. The RAG model then processes the query and provides answers, along with relevant log entries as evidence.\n",
    "\n",
    "def ask_question(question):\n",
    "    \"\"\"\n",
    "    Processes a user query using the RAG model and returns an answer with relevant source documents.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question about the web traffic logs.\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints the question, the generated answer, and source documents.\n",
    "    \"\"\"\n",
    "    result = rag_chain({\"query\": question})\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "    print(\"\\nSource Documents:\")\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"{i}. {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Anaconda\\envs\\RAG-SYSTEM\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Are there any unusual patterns in user-agent strings that might indicate bot activity or potential attackers?\n",
      "Answer: Based on the provided log entries, I have analyzed the user-agent strings to identify any unusual patterns. Here's my findings:\n",
      "\n",
      "The user-agent strings in all four log entries are identical and match a Googlebot/2.1 crawler. The presence of `compatible;` at the end of the string is also indicative of a crawler, as it suggests that this user agent is compatible with other services.\n",
      "\n",
      "The repeated occurrence of the same user-agent string across multiple log entries (specifically, from IP address 66.249.66.194) raises suspicions about potential bot activity orcrawler-based traffic. The fact that all four log entries have the same user-agent string and IP address suggests a consistent pattern, which could be indicative of automated crawling activity.\n",
      "\n",
      "To further support this conclusion, I would cross-reference these log entries with other logs to determine if there are any other occurrences of similar patterns. This would help to establish whether this is an isolated instance or part of a larger bot-related activity.\n",
      "\n",
      "In conclusion, based on the provided log entries, I believe that there might be unusual patterns in user-agent strings that could indicate bot activity or potential attackers. The repeated occurrence of the same user-agent string across multiple log entries from the same IP address suggests a consistent pattern that may warrant further investigation.\n",
      "\n",
      "Source Documents:\n",
      "1. GET /m/browse/hood/%D9%87%D9%88%D8%AF (Status: 200, Size: 18852, IP: 66.249.66.194) (Datetime: 2019-01-22 09:42:47+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWeb...\n",
      "2. GET /m/browse/hood/%D9%87%D9%88%D8%AF (Status: 200, Size: 19025, IP: 66.249.66.194) (Datetime: 2019-01-24 15:28:31+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWeb...\n",
      "3. GET /filter/b1,p62,stexists,v1%7CGold (Status: 302, Size: 0, IP: 66.249.66.194) (Datetime: 2019-01-23 07:00:56+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/...\n",
      "4. GET /m/browse/hood/%D9%87%D9%88%D8%AF (Status: 200, Size: 18887, IP: 66.249.66.194) (Datetime: 2019-01-25 20:53:42+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWeb...\n",
      "5. GET /search/%D8%AC%D8%A7%D8%B1/b193 (Status: 302, Size: 0, IP: 66.249.66.194) (Datetime: 2019-01-26 15:20:49+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/53...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which HTTP methods are predominantly used in the logs, and what does this tell us about the nature of the traffic?\n",
      "Answer: Based on the provided log entries, we can see that there are only two instances of different HTTP methods being used. The first is a GET request (200 OK), and the second is a POST request.\n",
      "\n",
      "Here are the relevant log entries:\n",
      "\n",
      "* GET requests:\n",
      "\t+ /blog/tag/%d8%aa%d8%b4%da%a9-%d8%b1%d9%88%db%8c%d8%a7/ (2019-01-26 17:05:25+03:30)\n",
      "\t+ /m/article/711/%DA%86%DA%AF%D9%88%D9%86%D9%87-%D8%A7%D8%B2-%D9%88%D8%A7%DA%A9%D8%B3-%D9%85%D9%88-%D8%A7%D8%B3%D8%AA%D9%81%D8%A7%D8%AF%D9%87-%DA%A9%D9%86%DB%8C%D9%85%D8%9F (2019-01-23 17:02:28+03:30)\n",
      "\t+ /m/article/122/%D8%B1%D8%A7%D9%87%D9%86%D9%85%D8%A7%DB%8C-%D8%AE%D8%B1%DB%8C%D8%AF-%D8%AA%D8%BE%D9%85-%D9%85%D8%B1%D8%BA-%D9%BE%D8%B2 (2019-01-25 18:10:40+03:30)\n",
      "\t+ /blog/home-appliances/%da%86%d8%b1%d9%88%da%a9-%d9%87%d8%a7%db%8c-%d9%84%d8%a8%d8%a7%d8%b3-%d8%b1%d8%a7-%d8%a7%d8%b2-%d8%a8%db%8c%d9%86-%d8%a8%d8%a8%d8%b1%db%8c%d8%af/ (2019-01-24 10:08:16+03:30)\n",
      "* POST request:\n",
      "\t+ /order/track (2019-01-23 13:44:44+03:30)\n",
      "\n",
      "The GET requests are the most common and account for all but one of the log entries. This suggests that the majority of traffic is focused on retrieving data or resources, rather than submitting data.\n",
      "\n",
      "The single POST request, which is used to track an order, indicates that there may be some transactional or interactive activity occurring, but it does not seem to be a dominant feature of the traffic.\n",
      "\n",
      "This analysis tells us that the nature of the traffic is primarily concerned with information retrieval and possibly some basic interactions.\n",
      "\n",
      "Source Documents:\n",
      "1. GET /blog/tag/%d8%aa%d8%b4%da%a9-%d8%b1%d9%88%db%8c%d8%a7/ (Status: 200, Size: 21004, IP: 87.251.148.46) (Datetime: 2019-01-26 17:05:25+03:30, User Agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537....\n",
      "2. GET /m/article/711/%DA%86%DA%AF%D9%88%D9%86%D9%87-%D8%A7%D8%B2-%D9%88%D8%A7%DA%A9%D8%B3-%D9%85%D9%88-%D8%A7%D8%B3%D8%AA%D9%81%D8%A7%D8%AF%D9%87-%DA%A9%D9%86%DB%8C%D9%85%D8%9F (Status: 200, Size: 16939...\n",
      "3. GET /m/article/122/%D8%B1%D8%A7%D9%87%D9%86%D9%85%D8%A7%DB%8C-%D8%AE%D8%B1%DB%8C%D8%AF-%D8%AA%D8%AE%D9%85-%D9%85%D8%B1%D8%BA-%D9%BE%D8%B2 (Status: 200, Size: 16040, IP: 66.249.66.194) (Datetime: 2019-...\n",
      "4. GET /blog/home-appliances/%da%86%d8%b1%d9%88%da%a9-%d9%87%d8%a7%db%8c-%d9%84%d8%a8%d8%a7%d8%b3-%d8%b1%d8%a7-%d8%a7%d8%b2-%d8%a8%db%8c%d9%86-%d8%a8%d8%a8%d8%b1%db%8c%d8%af/ (Status: 200, Size: 24351, I...\n",
      "5. POST /order/track (Status: 200, Size: 33877, IP: 91.98.43.158) (Datetime: 2019-01-23 13:44:44+03:30, User Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:65.0) Gecko/20100101 Firefox/65.0)...\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"Are there any unusual patterns in user-agent strings that might indicate bot activity or potential attackers?\")\n",
    "ask_question(\"Which HTTP methods are predominantly used in the logs, and what does this tell us about the nature of the traffic?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the Streamlit app\n",
    "!streamlit run streamlit_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
