{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.2 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: numpy==1.26.4 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: streamlit==1.1.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from -r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: torch==2.0.1 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from -r requirements.txt (line 4)) (2.0.1)\n",
      "Requirement already satisfied: transformers>=4.30.2 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from -r requirements.txt (line 5)) (4.44.1)\n",
      "Collecting langchain==0.2.12 (from -r requirements.txt (line 6))\n",
      "  Using cached langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: langchain-huggingface>=0.0.3 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from -r requirements.txt (line 7)) (0.0.3)\n",
      "Requirement already satisfied: langchain-ollama==0.1.1 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from -r requirements.txt (line 8)) (0.1.1)\n",
      "Requirement already satisfied: langchain-community in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from -r requirements.txt (line 9)) (0.2.12)\n",
      "Requirement already satisfied: tqdm==4.65.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from -r requirements.txt (line 10)) (4.65.0)\n",
      "Requirement already satisfied: faiss-cpu in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from -r requirements.txt (line 11)) (1.8.0.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: altair>=3.2.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (5.4.0)\n",
      "Requirement already satisfied: astor in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (0.8.1)\n",
      "Requirement already satisfied: attrs in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (24.2.0)\n",
      "Requirement already satisfied: base58 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: blinker in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (1.8.2)\n",
      "Requirement already satisfied: cachetools>=4.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (5.5.0)\n",
      "Requirement already satisfied: click<8.0,>=7.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (7.1.2)\n",
      "Requirement already satisfied: packaging in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (10.4.0)\n",
      "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (3.20.3)\n",
      "Requirement already satisfied: pyarrow in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (17.0.0)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (0.9.1)\n",
      "Requirement already satisfied: requests in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: toml in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (0.10.2)\n",
      "Requirement already satisfied: tornado>=5.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (6.2)\n",
      "Requirement already satisfied: tzlocal in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (5.2)\n",
      "Requirement already satisfied: validators in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (0.33.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (3.1.43)\n",
      "Requirement already satisfied: watchdog in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: filelock in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: sympy in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 4)) (1.13.2)\n",
      "Requirement already satisfied: networkx in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (0.2.34)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (0.1.101)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (8.5.0)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain-ollama==0.1.1->-r requirements.txt (line 8)) (0.3.1)\n",
      "Requirement already satisfied: colorama in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from tqdm==4.65.0->-r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from transformers>=4.30.2->-r requirements.txt (line 5)) (0.24.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from transformers>=4.30.2->-r requirements.txt (line 5)) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from transformers>=4.30.2->-r requirements.txt (line 5)) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from transformers>=4.30.2->-r requirements.txt (line 5)) (0.19.1)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain-huggingface>=0.0.3->-r requirements.txt (line 7)) (3.0.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain-community->-r requirements.txt (line 9)) (0.6.7)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-community (from -r requirements.txt (line 9))\n",
      "  Using cached langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12->-r requirements.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12->-r requirements.txt (line 6)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12->-r requirements.txt (line 6)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12->-r requirements.txt (line 6)) (1.9.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 3)) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.1.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 9)) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from gitpython!=3.1.19->streamlit==1.1.0->-r requirements.txt (line 3)) (4.0.11)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.30.2->-r requirements.txt (line 5)) (2024.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain==0.2.12->-r requirements.txt (line 6)) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.12->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.12->-r requirements.txt (line 6)) (2.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 4)) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from requests->streamlit==1.1.0->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from requests->streamlit==1.1.0->-r requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from requests->streamlit==1.1.0->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from requests->streamlit==1.1.0->-r requirements.txt (line 3)) (2024.7.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface>=0.0.3->-r requirements.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: scipy in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface>=0.0.3->-r requirements.txt (line 7)) (1.14.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.12->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==1.1.0->-r requirements.txt (line 3)) (5.0.1)\n",
      "Requirement already satisfied: anyio in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (1.0.5)\n",
      "Requirement already satisfied: sniffio in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain==0.2.12->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 3)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 3)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 3)) (0.20.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface>=0.0.3->-r requirements.txt (line 7)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\ai\\anaconda\\envs\\myproject\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface>=0.0.3->-r requirements.txt (line 7)) (3.5.0)\n",
      "Using cached langchain-0.2.12-py3-none-any.whl (990 kB)\n",
      "Using cached langchain_community-0.2.11-py3-none-any.whl (2.3 MB)\n",
      "Installing collected packages: langchain, langchain-community\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.14\n",
      "    Uninstalling langchain-0.2.14:\n",
      "      Successfully uninstalled langchain-0.2.14\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.2.12\n",
      "    Uninstalling langchain-community-0.2.12:\n",
      "      Successfully uninstalled langchain-community-0.2.12\n",
      "Successfully installed langchain-0.2.12 langchain-community-0.2.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Web Traffic Log-Based Q&A System\n",
    "# \n",
    "# This project uses web traffic logs to create a question-answering system powered by a Retrieval-Augmented Generation (RAG) model. The system processes logs, stores them in a vector database, and uses two different language models (LLaMA 3 and Google T5) to generate answers to user queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Anaconda\\envs\\myproject\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\"\"\"\n",
    "Import necessary libraries and modules for the Q&A BOT notebook.\n",
    "This code imports the following libraries and modules:\n",
    "- re: Regular expression operations.\n",
    "- pandas: Data manipulation and analysis.\n",
    "- numpy: Numerical computing.\n",
    "- datetime: Date and time manipulation.\n",
    "- faiss: Efficient similarity search and clustering of dense vectors.\n",
    "- tqdm: Progress bar for loops and tasks.\n",
    "- langchain.vectorstores: Vector stores for language embeddings.\n",
    "- langchain_huggingface: Hugging Face embeddings for language models.\n",
    "- langchain.docstore: Document store for storing and retrieving documents.\n",
    "- langchain.schema: Schema for defining document structure.\n",
    "- transformers: State-of-the-art natural language processing models.\n",
    "- langchain_huggingface: Hugging Face pipeline for language models.\n",
    "- langchain.chains: Retrieval-based question answering model.\n",
    "The code also checks for GPU availability, clears the GPU cache, and sets the device to either \"cuda\" or \"cpu\" based on availability.\n",
    "Note: The code assumes that the necessary libraries and modules are already installed.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import faiss\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.schema import Document\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Check for GPU availability\n",
    "import torch\n",
    "# Clear GPU cache before and after running the model\n",
    "torch.cuda.empty_cache()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log parsing and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents: [Document(metadata={'source': 'downsampled_logs.csv', 'row': 0}, page_content='IP: 5.211.192.11\\nIdentity: -\\nUser: -\\nTimestamp: 24/Jan/2019:18:14:27 +0330\\nRequest: GET /image/173/productTypeType HTTP/1.1\\nStatus: 200\\nSize: 10976\\nReferer: https://www.zanbil.ir/m/browse/analog-watch/%D8%B3%D8%A7%D8%B9%D8%AA-%D9%85%DA%86%DB%8C-%D8%B9%D9%82%D8%B1%D8%A8%D9%87-%D8%A7%DB%8C\\nUser-Agent: Mozilla/5.0 (Linux; Android 9; ONEPLUS A5010) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.99 Mobile Safari/537.36'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 1}, page_content='IP: 5.52.206.13\\nIdentity: -\\nUser: -\\nTimestamp: 22/Jan/2019:23:13:55 +0330\\nRequest: GET /static/images/guarantees/support.png HTTP/1.1\\nStatus: 200\\nSize: 6454\\nReferer: https://www.zanbil.ir/m/filter/p29%2Ct120?name=%D9%87%D9%85%D8%B2%D9%86&productType=mixer\\nUser-Agent: Mozilla/5.0 (iPhone; CPU iPhone OS 12_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) GSA/65.0.225212226 Mobile/15E148 Safari/605.1'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 2}, page_content='IP: 37.202.230.203\\nIdentity: -\\nUser: -\\nTimestamp: 23/Jan/2019:09:51:48 +0330\\nRequest: GET /product/33598/64267/ HTTP/1.1\\nStatus: 302\\nSize: 0\\nReferer: https://emalls.ir/%D8%AE%D8%B1%DB%8C%D8%AF_%D9%81%D8%B1%D9%88%D8%B4%DA%AF%D8%A7%D9%87-%D8%A7%DB%8C%D9%86%D8%AA%D8%B1%D9%86%D8%AA%DB%8C-%D8%B2%D9%86%D8%A8%DB%8C%D9%84~eitemid~1728829~exid~14392366\\nUser-Agent: Mozilla/5.0 (Linux; Android 8.0.0; SM-A520F Build/R16NW) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.91 Mobile Safari/537.36'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 3}, page_content='IP: 89.144.190.26\\nIdentity: -\\nUser: -\\nTimestamp: 25/Jan/2019:20:15:03 +0330\\nRequest: GET /image/32/brand HTTP/1.1\\nStatus: 200\\nSize: 2161\\nReferer: https://www.zanbil.ir/browse/home-appliances/%D9%84%D9%88%D8%A7%D8%B2%D9%85-%D8%AE%D8%A7%D9%86%DA%AF%DB%8C\\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20100101 Firefox/64.0'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 4}, page_content='IP: 37.98.115.133\\nIdentity: -\\nUser: -\\nTimestamp: 25/Jan/2019:07:33:43 +0330\\nRequest: GET /image/543/brand HTTP/1.1\\nStatus: 200\\nSize: 2783\\nReferer: https://www.zanbil.ir/filter/b36\\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0')]\n",
      "DataFrame head:                ip identity user                    datetime method  \\\n",
      "0    5.211.192.11        -    -  24/Jan/2019:18:14:27 +0330    GET   \n",
      "1     5.52.206.13        -    -  22/Jan/2019:23:13:55 +0330    GET   \n",
      "2  37.202.230.203        -    -  23/Jan/2019:09:51:48 +0330    GET   \n",
      "3   89.144.190.26        -    -  25/Jan/2019:20:15:03 +0330    GET   \n",
      "4   37.98.115.133        -    -  25/Jan/2019:07:33:43 +0330    GET   \n",
      "\n",
      "                                     url status   size  \\\n",
      "0             /image/173/productTypeType    200  10976   \n",
      "1  /static/images/guarantees/support.png    200   6454   \n",
      "2                  /product/33598/64267/    302      0   \n",
      "3                        /image/32/brand    200   2161   \n",
      "4                       /image/543/brand    200   2783   \n",
      "\n",
      "                                             referer  \\\n",
      "0  https://www.zanbil.ir/m/browse/analog-watch/%D...   \n",
      "1  https://www.zanbil.ir/m/filter/p29%2Ct120?name...   \n",
      "2  https://emalls.ir/%D8%AE%D8%B1%DB%8C%D8%AF_%D9...   \n",
      "3  https://www.zanbil.ir/browse/home-appliances/%...   \n",
      "4                   https://www.zanbil.ir/filter/b36   \n",
      "\n",
      "                                          user_agent  \n",
      "0  Mozilla/5.0 (Linux; Android 9; ONEPLUS A5010) ...  \n",
      "1  Mozilla/5.0 (iPhone; CPU iPhone OS 12_0 like M...  \n",
      "2  Mozilla/5.0 (Linux; Android 8.0.0; SM-A520F Bu...  \n",
      "3  Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...  \n",
      "4  Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:64...  \n",
      "               ip identity user                  datetime method  \\\n",
      "0    5.211.192.11        -    - 2019-01-24 18:14:27+03:30    GET   \n",
      "1     5.52.206.13        -    - 2019-01-22 23:13:55+03:30    GET   \n",
      "2  37.202.230.203        -    - 2019-01-23 09:51:48+03:30    GET   \n",
      "3   89.144.190.26        -    - 2019-01-25 20:15:03+03:30    GET   \n",
      "4   37.98.115.133        -    - 2019-01-25 07:33:43+03:30    GET   \n",
      "\n",
      "                                     url  status   size  \\\n",
      "0             /image/173/productTypeType     200  10976   \n",
      "1  /static/images/guarantees/support.png     200   6454   \n",
      "2                  /product/33598/64267/     302      0   \n",
      "3                        /image/32/brand     200   2161   \n",
      "4                       /image/543/brand     200   2783   \n",
      "\n",
      "                                             referer  \\\n",
      "0  https://www.zanbil.ir/m/browse/analog-watch/%D...   \n",
      "1  https://www.zanbil.ir/m/filter/p29%2Ct120?name...   \n",
      "2  https://emalls.ir/%D8%AE%D8%B1%DB%8C%D8%AF_%D9...   \n",
      "3  https://www.zanbil.ir/browse/home-appliances/%...   \n",
      "4                   https://www.zanbil.ir/filter/b36   \n",
      "\n",
      "                                          user_agent  hour  day  month  year  \\\n",
      "0  Mozilla/5.0 (Linux; Android 9; ONEPLUS A5010) ...    18   24      1  2019   \n",
      "1  Mozilla/5.0 (iPhone; CPU iPhone OS 12_0 like M...    23   22      1  2019   \n",
      "2  Mozilla/5.0 (Linux; Android 8.0.0; SM-A520F Bu...     9   23      1  2019   \n",
      "3  Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...    20   25      1  2019   \n",
      "4  Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:64...     7   25      1  2019   \n",
      "\n",
      "   weekday  status_category                                               text  \n",
      "0        3                2  GET /image/173/productTypeType (Status: 200, S...  \n",
      "1        1                2  GET /static/images/guarantees/support.png (Sta...  \n",
      "2        2                3  GET /product/33598/64267/ (Status: 302, Size: ...  \n",
      "3        4                2  GET /image/32/brand (Status: 200, Size: 2161, ...  \n",
      "4        4                2  GET /image/543/brand (Status: 200, Size: 2783,...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import CSVLoader\n",
    "import re\n",
    "# The following code handles the loading, parsing, and preprocessing of web traffic logs. The logs are parsed to extract useful fields, and the data is then cleaned and structured for further use.\n",
    "\n",
    "# Function to parse the page content\n",
    "def parse_page_content(page_content):\n",
    "    \"\"\"\n",
    "    Parses the content of a web traffic log entry.\n",
    "    \n",
    "    Args:\n",
    "        page_content (str): A single log entry as a string.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing parsed fields like IP, identity, user, datetime, method, etc.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r'IP: (?P<ip>[\\d\\.]+)\\n'\n",
    "        r'Identity: (?P<identity>.+)\\n'\n",
    "        r'User: (?P<user>.+)\\n'\n",
    "        r'Timestamp: (?P<datetime>.+)\\n'\n",
    "        r'Request: (?P<method>\\w+) (?P<url>.+) HTTP/\\d\\.\\d\\n'\n",
    "        r'Status: (?P<status>\\d+)\\n'\n",
    "        r'Size: (?P<size>\\d+)\\n'\n",
    "        r'Referer: (?P<referer>.+)\\n'\n",
    "        r'User-Agent: (?P<user_agent>.+)'\n",
    "    )\n",
    "    match = pattern.search(page_content)\n",
    "    if match:\n",
    "        return match.groupdict()\n",
    "    return {}\n",
    "\n",
    "# Function to downsample the CSV file\n",
    "def downsample_csv(csv_file_path, sample_size, output_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    if sample_size and sample_size < len(df):\n",
    "        df = df.sample(n=sample_size)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    return output_file_path\n",
    "\n",
    "# Load and preprocess logs using CSVLoader\n",
    "def load_and_preprocess_logs(csv_file_path, sample_size=10000):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses web traffic logs from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path (str): Path to the CSV file containing log data.\n",
    "        sample_size (int): Number of samples to load from the file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing processed log data with additional fields for analysis.\n",
    "    \"\"\"\n",
    "    # Downsample the CSV file\n",
    "    downsampled_csv = downsample_csv(csv_file_path, sample_size, 'downsampled_logs.csv')\n",
    "    \n",
    "    # Load the downsampled CSV file\n",
    "    loader = CSVLoader(file_path=downsampled_csv)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Debug: Print the first few documents to check if they are loaded correctly\n",
    "    print(\"Loaded documents:\", documents[:5])\n",
    "    \n",
    "    # Convert documents to DataFrame\n",
    "    data = []\n",
    "    for doc in documents:\n",
    "        parsed_data = parse_page_content(doc.page_content)\n",
    "        data.append(parsed_data)\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Debug: Print the DataFrame to check if it is populated correctly\n",
    "    print(\"DataFrame head:\", df.head())\n",
    "    \n",
    "    # Preprocess logs\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format='%d/%b/%Y:%H:%M:%S %z', errors='coerce')\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['weekday'] = df['datetime'].dt.weekday\n",
    "    df['status'] = df['status'].astype(int, errors='ignore')\n",
    "    df['size'] = df['size'].astype(int, errors='ignore')\n",
    "    df['status_category'] = df['status'] // 100\n",
    "    \n",
    "    # Create a text field for embedding\n",
    "    df['text'] = df.apply(lambda row: f\"{row['method']} {row['url']} (Status: {row['status']}, Size: {row['size']}, IP: {row['ip']})\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "processed_logs = load_and_preprocess_logs('../processed_logs_sample.csv', sample_size=10000)\n",
    "\n",
    "print(processed_logs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Anaconda\\envs\\myproject\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# The following section creates a vector database using FAISS and stores the processed log data for efficient retrieval during query processing.\n",
    "\n",
    "\"\"\"\n",
    "Initialize the embedding model using HuggingFaceEmbeddings.\n",
    "Parameters:\n",
    "- model_name (str): The name of the Hugging Face model to use for embeddings.\n",
    "Returns:\n",
    "- embeddings (HuggingFaceEmbeddings): The initialized HuggingFaceEmbeddings object.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Create documents for vector store.\n",
    "Parameters:\n",
    "- processed_logs (DataFrame): The processed logs containing 'text', 'datetime', and 'user_agent' columns.\n",
    "Returns:\n",
    "- documents (list): The list of documents for the vector store, with additional information appended.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Split documents into smaller chunks.\n",
    "Parameters:\n",
    "- chunk_size (int): The size of each chunk.\n",
    "- chunk_overlap (int): The overlap between consecutive chunks.\n",
    "Returns:\n",
    "- texts (list): The list of split documents.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Create the vector store using FAISS.\n",
    "Parameters:\n",
    "- texts (list): The list of split documents.\n",
    "- embeddings (HuggingFaceEmbeddings): The initialized HuggingFaceEmbeddings object.\n",
    "Returns:\n",
    "- vectorstore (FAISS): The created FAISS vector store.\n",
    "\"\"\"\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create documents for vector store\n",
    "documents = [\n",
    "    f\"{row['text']} (Datetime: {row['datetime']}, User Agent: {row['user_agent']})\"\n",
    "    for _, row in processed_logs.iterrows()\n",
    "]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.create_documents(documents)\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rag Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Anaconda\\envs\\myproject\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\"\"\"\n",
    "Create a RAG (Retrieval-Augmented Generation) chain for question answering.\n",
    "Args:\n",
    "    llm (HuggingFacePipeline): The HuggingFace pipeline for text generation.\n",
    "    chain_type (str): The type of chain to create.\n",
    "    retriever (Retriever): The retriever used for document retrieval.\n",
    "    chain_type_kwargs (dict): Additional keyword arguments specific to the chain type.\n",
    "Returns:\n",
    "    RetrievalQA: The created RAG chain for question answering.\n",
    "\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=512,\n",
    "    device=0 if device == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# The following function allows users to input questions about the web traffic logs. The RAG model then processes the query and provides answers, along with relevant log entries as evidence.\n",
    "\n",
    "def ask_question(question):\n",
    "    \"\"\"\n",
    "    Processes a user query using the RAG model and returns an answer with relevant source documents.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question about the web traffic logs.\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints the question, the generated answer, and source documents.\n",
    "    \"\"\"\n",
    "    result = rag_chain({\"query\": question})\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "    print(\"\\nSource Documents:\")\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"{i}. {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Anaconda\\envs\\myproject\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (756 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Are there any unusual patterns in user-agent strings that might indicate bot activity or potential attackers?\n",
      "Answer: Yes.\n",
      "\n",
      "Source Documents:\n",
      "1. GET /m/filter/b258,p8870 (Status: 200, Size: 18388, IP: 66.249.66.194) (Datetime: 2019-01-23 13:45:47+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (K...\n",
      "2. GET /m/filter/b99,p5 (Status: 200, Size: 18619, IP: 66.249.66.194) (Datetime: 2019-01-23 12:17:48+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML...\n",
      "3. GET /m/filter/p7489,t194 (Status: 200, Size: 16746, IP: 66.249.66.194) (Datetime: 2019-01-25 04:19:10+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (K...\n",
      "4. GET /m/filter/b74,p56 (Status: 200, Size: 18415, IP: 66.249.66.194) (Datetime: 2019-01-22 13:45:12+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTM...\n",
      "5. GET /m/filter/b213,b41,p1 (Status: 200, Size: 21614, IP: 66.249.66.194) (Datetime: 2019-01-25 20:34:30+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (...\n",
      "Question: Which HTTP methods are predominantly used in the logs, and what does this tell us about the nature of the traffic?\n",
      "Answer: GET /m/filter/b2,p3\n",
      "\n",
      "Source Documents:\n",
      "1. GET /product/31349/61292/%D8%B3%D8%A7%D8%B9%D8%AA-%D9%85%DA%86%DB%8C-%D8%B9%D9%82%D8%B1%D8%A8%D9%87-%D8%A7%DB%8C-%D8%B2%D9%86%D8%A7%D9%86%D9%87-TISSOT-%D9%85%D8%AF%D9%84-T085.210.22.013.00 (Status: 20...\n",
      "2. GET /m/filter/b2,p3 (Status: 200, Size: 19538, IP: 83.121.230.255) (Datetime: 2019-01-24 20:35:51+03:30, User Agent: Mozilla/5.0 (Linux; Android 5.1.1; SM-J320F) AppleWebKit/537.36 (KHTML, like Gecko)...\n",
      "3. GET /filter/b211%2Cb502%2Cb214%2Cb389%2Cb335%2Cb554%2Cb265%2Cb549%2Cb142%2Cb487%2Cb153%2Cb579%2Cb428%2Cb412%2Cb353%2Cb307%2Cb742%2Cb95 (Status: 200, Size: 37587, IP: 17.58.102.43) (Datetime: 2019-01-2...\n",
      "4. GET /browse/home-appliances/%D9%84%D9%88%D8%A7%D8%B2%D9%85-%D8%AE%D8%A7%D9%86%DA%AF%DB%8C (Status: 499, Size: 0, IP: 5.113.215.131) (Datetime: 2019-01-23 22:41:16+03:30, User Agent: Mozilla/5.0 (Windo...\n",
      "5. GET /site/enamad (Status: 200, Size: 278, IP: 85.185.254.50) (Datetime: 2019-01-24 15:44:45+03:30, User Agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 S...\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"Are there any unusual patterns in user-agent strings that might indicate bot activity or potential attackers?\")\n",
    "ask_question(\"Which HTTP methods are predominantly used in the logs, and what does this tell us about the nature of the traffic?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the Streamlit app\n",
    "!streamlit run streamlit_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
