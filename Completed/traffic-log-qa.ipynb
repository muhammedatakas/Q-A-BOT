{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement re (from versions: none)\n",
      "ERROR: No matching distribution found for re\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Web Traffic Log-Based Q&A System\n",
    "# \n",
    "# This project uses web traffic logs to create a question-answering system powered by a Retrieval-Augmented Generation (RAG) model. The system processes logs, stores them in a vector database, and uses two different language models (LLaMA 3 and Google T5) to generate answers to user queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\"\"\"\n",
    "Import necessary libraries and modules for the Q&A BOT notebook.\n",
    "This code imports the following libraries and modules:\n",
    "- re: Regular expression operations.\n",
    "- pandas: Data manipulation and analysis.\n",
    "- numpy: Numerical computing.\n",
    "- datetime: Date and time manipulation.\n",
    "- faiss: Efficient similarity search and clustering of dense vectors.\n",
    "- tqdm: Progress bar for loops and tasks.\n",
    "- langchain.vectorstores: Vector stores for language embeddings.\n",
    "- langchain_huggingface: Hugging Face embeddings for language models.\n",
    "- langchain.docstore: Document store for storing and retrieving documents.\n",
    "- langchain.schema: Schema for defining document structure.\n",
    "- transformers: State-of-the-art natural language processing models.\n",
    "- langchain_huggingface: Hugging Face pipeline for language models.\n",
    "- langchain.chains: Retrieval-based question answering model.\n",
    "The code also checks for GPU availability, clears the GPU cache, and sets the device to either \"cuda\" or \"cpu\" based on availability.\n",
    "Note: The code assumes that the necessary libraries and modules are already installed.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import faiss\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.schema import Document\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Check for GPU availability\n",
    "import torch\n",
    "# Clear GPU cache before and after running the model\n",
    "torch.cuda.empty_cache()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log parsing and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents: [Document(metadata={'source': 'downsampled_logs.csv', 'row': 0}, page_content='IP: 5.211.192.11\\nIdentity: -\\nUser: -\\nTimestamp: 24/Jan/2019:18:14:27 +0330\\nRequest: GET /image/173/productTypeType HTTP/1.1\\nStatus: 200\\nSize: 10976\\nReferer: https://www.zanbil.ir/m/browse/analog-watch/%D8%B3%D8%A7%D8%B9%D8%AA-%D9%85%DA%86%DB%8C-%D8%B9%D9%82%D8%B1%D8%A8%D9%87-%D8%A7%DB%8C\\nUser-Agent: Mozilla/5.0 (Linux; Android 9; ONEPLUS A5010) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.99 Mobile Safari/537.36'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 1}, page_content='IP: 5.52.206.13\\nIdentity: -\\nUser: -\\nTimestamp: 22/Jan/2019:23:13:55 +0330\\nRequest: GET /static/images/guarantees/support.png HTTP/1.1\\nStatus: 200\\nSize: 6454\\nReferer: https://www.zanbil.ir/m/filter/p29%2Ct120?name=%D9%87%D9%85%D8%B2%D9%86&productType=mixer\\nUser-Agent: Mozilla/5.0 (iPhone; CPU iPhone OS 12_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) GSA/65.0.225212226 Mobile/15E148 Safari/605.1'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 2}, page_content='IP: 37.202.230.203\\nIdentity: -\\nUser: -\\nTimestamp: 23/Jan/2019:09:51:48 +0330\\nRequest: GET /product/33598/64267/ HTTP/1.1\\nStatus: 302\\nSize: 0\\nReferer: https://emalls.ir/%D8%AE%D8%B1%DB%8C%D8%AF_%D9%81%D8%B1%D9%88%D8%B4%DA%AF%D8%A7%D9%87-%D8%A7%DB%8C%D9%86%D8%AA%D8%B1%D9%86%D8%AA%DB%8C-%D8%B2%D9%86%D8%A8%DB%8C%D9%84~eitemid~1728829~exid~14392366\\nUser-Agent: Mozilla/5.0 (Linux; Android 8.0.0; SM-A520F Build/R16NW) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.91 Mobile Safari/537.36'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 3}, page_content='IP: 89.144.190.26\\nIdentity: -\\nUser: -\\nTimestamp: 25/Jan/2019:20:15:03 +0330\\nRequest: GET /image/32/brand HTTP/1.1\\nStatus: 200\\nSize: 2161\\nReferer: https://www.zanbil.ir/browse/home-appliances/%D9%84%D9%88%D8%A7%D8%B2%D9%85-%D8%AE%D8%A7%D9%86%DA%AF%DB%8C\\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20100101 Firefox/64.0'), Document(metadata={'source': 'downsampled_logs.csv', 'row': 4}, page_content='IP: 37.98.115.133\\nIdentity: -\\nUser: -\\nTimestamp: 25/Jan/2019:07:33:43 +0330\\nRequest: GET /image/543/brand HTTP/1.1\\nStatus: 200\\nSize: 2783\\nReferer: https://www.zanbil.ir/filter/b36\\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0')]\n",
      "DataFrame head:                ip identity user                    datetime method  \\\n",
      "0    5.211.192.11        -    -  24/Jan/2019:18:14:27 +0330    GET   \n",
      "1     5.52.206.13        -    -  22/Jan/2019:23:13:55 +0330    GET   \n",
      "2  37.202.230.203        -    -  23/Jan/2019:09:51:48 +0330    GET   \n",
      "3   89.144.190.26        -    -  25/Jan/2019:20:15:03 +0330    GET   \n",
      "4   37.98.115.133        -    -  25/Jan/2019:07:33:43 +0330    GET   \n",
      "\n",
      "                                     url status   size  \\\n",
      "0             /image/173/productTypeType    200  10976   \n",
      "1  /static/images/guarantees/support.png    200   6454   \n",
      "2                  /product/33598/64267/    302      0   \n",
      "3                        /image/32/brand    200   2161   \n",
      "4                       /image/543/brand    200   2783   \n",
      "\n",
      "                                             referer  \\\n",
      "0  https://www.zanbil.ir/m/browse/analog-watch/%D...   \n",
      "1  https://www.zanbil.ir/m/filter/p29%2Ct120?name...   \n",
      "2  https://emalls.ir/%D8%AE%D8%B1%DB%8C%D8%AF_%D9...   \n",
      "3  https://www.zanbil.ir/browse/home-appliances/%...   \n",
      "4                   https://www.zanbil.ir/filter/b36   \n",
      "\n",
      "                                          user_agent  \n",
      "0  Mozilla/5.0 (Linux; Android 9; ONEPLUS A5010) ...  \n",
      "1  Mozilla/5.0 (iPhone; CPU iPhone OS 12_0 like M...  \n",
      "2  Mozilla/5.0 (Linux; Android 8.0.0; SM-A520F Bu...  \n",
      "3  Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...  \n",
      "4  Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:64...  \n",
      "               ip identity user                  datetime method  \\\n",
      "0    5.211.192.11        -    - 2019-01-24 18:14:27+03:30    GET   \n",
      "1     5.52.206.13        -    - 2019-01-22 23:13:55+03:30    GET   \n",
      "2  37.202.230.203        -    - 2019-01-23 09:51:48+03:30    GET   \n",
      "3   89.144.190.26        -    - 2019-01-25 20:15:03+03:30    GET   \n",
      "4   37.98.115.133        -    - 2019-01-25 07:33:43+03:30    GET   \n",
      "\n",
      "                                     url  status   size  \\\n",
      "0             /image/173/productTypeType     200  10976   \n",
      "1  /static/images/guarantees/support.png     200   6454   \n",
      "2                  /product/33598/64267/     302      0   \n",
      "3                        /image/32/brand     200   2161   \n",
      "4                       /image/543/brand     200   2783   \n",
      "\n",
      "                                             referer  \\\n",
      "0  https://www.zanbil.ir/m/browse/analog-watch/%D...   \n",
      "1  https://www.zanbil.ir/m/filter/p29%2Ct120?name...   \n",
      "2  https://emalls.ir/%D8%AE%D8%B1%DB%8C%D8%AF_%D9...   \n",
      "3  https://www.zanbil.ir/browse/home-appliances/%...   \n",
      "4                   https://www.zanbil.ir/filter/b36   \n",
      "\n",
      "                                          user_agent  hour  day  month  year  \\\n",
      "0  Mozilla/5.0 (Linux; Android 9; ONEPLUS A5010) ...    18   24      1  2019   \n",
      "1  Mozilla/5.0 (iPhone; CPU iPhone OS 12_0 like M...    23   22      1  2019   \n",
      "2  Mozilla/5.0 (Linux; Android 8.0.0; SM-A520F Bu...     9   23      1  2019   \n",
      "3  Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...    20   25      1  2019   \n",
      "4  Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:64...     7   25      1  2019   \n",
      "\n",
      "   weekday  status_category                                               text  \n",
      "0        3                2  GET /image/173/productTypeType (Status: 200, S...  \n",
      "1        1                2  GET /static/images/guarantees/support.png (Sta...  \n",
      "2        2                3  GET /product/33598/64267/ (Status: 302, Size: ...  \n",
      "3        4                2  GET /image/32/brand (Status: 200, Size: 2161, ...  \n",
      "4        4                2  GET /image/543/brand (Status: 200, Size: 2783,...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import CSVLoader\n",
    "import re\n",
    "# The following code handles the loading, parsing, and preprocessing of web traffic logs. The logs are parsed to extract useful fields, and the data is then cleaned and structured for further use.\n",
    "\n",
    "# Function to parse the page content\n",
    "def parse_page_content(page_content):\n",
    "    \"\"\"\n",
    "    Parses the content of a web traffic log entry.\n",
    "    \n",
    "    Args:\n",
    "        page_content (str): A single log entry as a string.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing parsed fields like IP, identity, user, datetime, method, etc.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r'IP: (?P<ip>[\\d\\.]+)\\n'\n",
    "        r'Identity: (?P<identity>.+)\\n'\n",
    "        r'User: (?P<user>.+)\\n'\n",
    "        r'Timestamp: (?P<datetime>.+)\\n'\n",
    "        r'Request: (?P<method>\\w+) (?P<url>.+) HTTP/\\d\\.\\d\\n'\n",
    "        r'Status: (?P<status>\\d+)\\n'\n",
    "        r'Size: (?P<size>\\d+)\\n'\n",
    "        r'Referer: (?P<referer>.+)\\n'\n",
    "        r'User-Agent: (?P<user_agent>.+)'\n",
    "    )\n",
    "    match = pattern.search(page_content)\n",
    "    if match:\n",
    "        return match.groupdict()\n",
    "    return {}\n",
    "\n",
    "# Function to downsample the CSV file\n",
    "def downsample_csv(csv_file_path, sample_size, output_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    if sample_size and sample_size < len(df):\n",
    "        df = df.sample(n=sample_size)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    return output_file_path\n",
    "\n",
    "# Load and preprocess logs using CSVLoader\n",
    "def load_and_preprocess_logs(csv_file_path, sample_size=10000):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses web traffic logs from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path (str): Path to the CSV file containing log data.\n",
    "        sample_size (int): Number of samples to load from the file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing processed log data with additional fields for analysis.\n",
    "    \"\"\"\n",
    "    # Downsample the CSV file\n",
    "    downsampled_csv = downsample_csv(csv_file_path, sample_size, 'downsampled_logs.csv')\n",
    "    \n",
    "    # Load the downsampled CSV file\n",
    "    loader = CSVLoader(file_path=downsampled_csv)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Debug: Print the first few documents to check if they are loaded correctly\n",
    "    print(\"Loaded documents:\", documents[:5])\n",
    "    \n",
    "    # Convert documents to DataFrame\n",
    "    data = []\n",
    "    for doc in documents:\n",
    "        parsed_data = parse_page_content(doc.page_content)\n",
    "        data.append(parsed_data)\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Debug: Print the DataFrame to check if it is populated correctly\n",
    "    print(\"DataFrame head:\", df.head())\n",
    "    \n",
    "    # Preprocess logs\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format='%d/%b/%Y:%H:%M:%S %z', errors='coerce')\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['weekday'] = df['datetime'].dt.weekday\n",
    "    df['status'] = df['status'].astype(int, errors='ignore')\n",
    "    df['size'] = df['size'].astype(int, errors='ignore')\n",
    "    df['status_category'] = df['status'] // 100\n",
    "    \n",
    "    # Create a text field for embedding\n",
    "    df['text'] = df.apply(lambda row: f\"{row['method']} {row['url']} (Status: {row['status']}, Size: {row['size']}, IP: {row['ip']})\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "processed_logs = load_and_preprocess_logs('../processed_logs_sample.csv', sample_size=10000)\n",
    "\n",
    "print(processed_logs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Anaconda\\envs\\RAG-SYSTEM\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "d:\\AI\\Anaconda\\envs\\RAG-SYSTEM\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# The following section creates a vector database using FAISS and stores the processed log data for efficient retrieval during query processing.\n",
    "\n",
    "\"\"\"\n",
    "Initialize the embedding model using HuggingFaceEmbeddings.\n",
    "Parameters:\n",
    "- model_name (str): The name of the Hugging Face model to use for embeddings.\n",
    "Returns:\n",
    "- embeddings (HuggingFaceEmbeddings): The initialized HuggingFaceEmbeddings object.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Create documents for vector store.\n",
    "Parameters:\n",
    "- processed_logs (DataFrame): The processed logs containing 'text', 'datetime', and 'user_agent' columns.\n",
    "Returns:\n",
    "- documents (list): The list of documents for the vector store, with additional information appended.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Split documents into smaller chunks.\n",
    "Parameters:\n",
    "- chunk_size (int): The size of each chunk.\n",
    "- chunk_overlap (int): The overlap between consecutive chunks.\n",
    "Returns:\n",
    "- texts (list): The list of split documents.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Create the vector store using FAISS.\n",
    "Parameters:\n",
    "- texts (list): The list of split documents.\n",
    "- embeddings (HuggingFaceEmbeddings): The initialized HuggingFaceEmbeddings object.\n",
    "Returns:\n",
    "- vectorstore (FAISS): The created FAISS vector store.\n",
    "\"\"\"\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create documents for vector store\n",
    "documents = [\n",
    "    f\"{row['text']} (Datetime: {row['datetime']}, User Agent: {row['user_agent']})\"\n",
    "    for _, row in processed_logs.iterrows()\n",
    "]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.create_documents(documents)\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rag Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# The RAG (Retrieval-Augmented Generation) model combines retrieval from the vector store and language generation using the LLaMA 3 model. It is set up here to handle user queries.\n",
    "\n",
    "\n",
    "\"\"\"\"\"\n",
    "This code initializes a retrieval-based question answering (QA) system for analyzing web traffic log entries. The system uses the OllamaLLM language model and the RetrievalQA chain from the langchain library.\n",
    "The main steps in the code are as follows:\n",
    "1. Set up logging for the system.\n",
    "2. Initialize the OllamaLLM language model with specific parameters.\n",
    "3. Define a template for generating prompts for the QA system.\n",
    "4. Create a PromptTemplate object with the defined template and input variables.\n",
    "5. Create a RetrievalQA chain using the OllamaLLM model, a retriever, and the PromptTemplate.\n",
    "6. The RetrievalQA chain is ready to be used for answering questions based on the provided log entries.\n",
    ".\"\"\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize OllamaLLM\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3\",  # or the specific model you're using\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "template = \"\"\"\"\n",
    "    You are an expert cybersecurity analyst specializing in web traffic log analysis. You have access to detailed log entries from a high-traffic website. Your task is to provide an in-depth analysis of these logs to answer specific questions posed by users.\n",
    "\n",
    "    When analyzing the logs, follow these guidelines:\n",
    "    1. **Data Integrity:** Ensure the accuracy and completeness of the data by cross-referencing multiple log entries where applicable.\n",
    "    2. **Pattern Recognition:** Identify and explain any significant patterns or anomalies in user behavior, such as:\n",
    "       - Repeated access from specific IP addresses.\n",
    "       - Unusual patterns in user-agent strings (indicating bots, crawlers, or potential attackers).\n",
    "       - Consistent access to specific pages or endpoints at unusual times.\n",
    "    3. **Contextual Correlation:** Correlate log entries across different dimensions (e.g., IP, time, URL) to build a coherent narrative of the events.\n",
    "    4. **Security Implications:** Assess and highlight any potential security concerns, such as:\n",
    "       - Signs of DDoS attacks.\n",
    "       - Unusual traffic spikes that could indicate brute force attempts.\n",
    "       - Access patterns that suggest vulnerability scanning.\n",
    "    5. **Detailed Justification:** For each observation or conclusion, provide specific log entries as evidence. Explain how these logs lead to your conclusions.\n",
    "\n",
    "    If the information requested is not available in the logs or if you are unable to determine an answer, clearly state that the data is inconclusive.\n",
    "\n",
    "    Below are the relevant log entries:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Based on the above logs, answer the following question with detailed reasoning and examples:\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# The following function allows users to input questions about the web traffic logs. The RAG model then processes the query and provides answers, along with relevant log entries as evidence.\n",
    "\n",
    "def ask_question(question):\n",
    "    \"\"\"\n",
    "    Processes a user query using the RAG model and returns an answer with relevant source documents.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question about the web traffic logs.\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints the question, the generated answer, and source documents.\n",
    "    \"\"\"\n",
    "    result = rag_chain({\"query\": question})\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "    print(\"\\nSource Documents:\")\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"{i}. {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Anaconda\\envs\\RAG-SYSTEM\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Are there any unusual patterns in user-agent strings that might indicate bot activity or potential attackers?\n",
      "Answer: Based on the provided log entries, I found an unusual pattern in the User-Agent strings that suggests potential bot activity. Specifically, all log entries have identical User-Agent strings:\n",
      "\n",
      "`Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.96 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)`\n",
      "\n",
      "This User-Agent string is not typical of human users, as it identifies the browser as a mobile device and includes references to Googlebot. Googlebot is a web scraping bot used by Google to crawl websites for indexing purposes.\n",
      "\n",
      "Given that all log entries have the same User-Agent string, I conclude that these requests are likely from bots or crawlers, rather than human users. The consistency of this pattern across multiple log entries suggests automated activity.\n",
      "\n",
      "**Data Integrity:** To ensure data integrity, I cross-referenced multiple log entries and found no discrepancies in the User-Agent strings. This strengthens my conclusion about bot activity.\n",
      "\n",
      "**Pattern Recognition:** I recognized a significant pattern in the User-Agent strings that indicates potential bot or crawler activity.\n",
      "\n",
      "**Contextual Correlation:** By correlating the log entries across different dimensions (IP, time, URL), I found that all requests came from the same IP address (`66.249.66.194`) and were made to specific URLs (`/m/filter/*`). This suggests a focused crawling or scraping effort by these bots.\n",
      "\n",
      "**Security Implications:** The presence of bot activity may indicate potential security concerns, such as:\n",
      "\n",
      "1. **Vulnerability scanning**: Bots may be attempting to identify vulnerabilities in the website's infrastructure or applications.\n",
      "2. **Data extraction**: These bots might be harvesting data from the website for malicious purposes.\n",
      "3. **DDoS attacks**: In some cases, botnets can be used to launch distributed denial-of-service (DDoS) attacks against a target website.\n",
      "\n",
      "**Detailed Justification:** The log entries themselves demonstrate the consistency of this User-Agent string:\n",
      "\n",
      "* GET /m/filter/b258,p8870 (Status: 200, Size: 18388, IP: 66.249.66.194) (Datetime: 2019-01-23 13:45:47+03:30, User Agent: ... )\n",
      "\n",
      "* ...\n",
      "\n",
      "All log entries share the same User-Agent string, which is unusual and suggests automated activity.\n",
      "\n",
      "In conclusion, based on the analysis of these logs, I found an unusual pattern in the User-Agent strings that indicates potential bot activity or attacker presence.\n",
      "\n",
      "Source Documents:\n",
      "1. GET /m/filter/b258,p8870 (Status: 200, Size: 18388, IP: 66.249.66.194) (Datetime: 2019-01-23 13:45:47+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (K...\n",
      "2. GET /m/filter/b99,p5 (Status: 200, Size: 18619, IP: 66.249.66.194) (Datetime: 2019-01-23 12:17:48+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML...\n",
      "3. GET /m/filter/p7489,t194 (Status: 200, Size: 16746, IP: 66.249.66.194) (Datetime: 2019-01-25 04:19:10+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (K...\n",
      "4. GET /m/filter/b74,p56 (Status: 200, Size: 18415, IP: 66.249.66.194) (Datetime: 2019-01-22 13:45:12+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTM...\n",
      "5. GET /m/filter/b213,b41,p1 (Status: 200, Size: 21614, IP: 66.249.66.194) (Datetime: 2019-01-25 20:34:30+03:30, User Agent: Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which HTTP methods are predominantly used in the logs, and what does this tell us about the nature of the traffic?\n",
      "Answer: Based on the provided log entries, we can observe that all requests are using the GET method. Here's a breakdown of the log entries:\n",
      "\n",
      "1. `GET /product/31349/61292/...`\n",
      "2. `GET /m/filter/b2,p3`\n",
      "3. `GET /filter/b211,...`\n",
      "4. `GET /browse/home-appliances/...`\n",
      "5. `GET /site/enamad`\n",
      "\n",
      "All five log entries are using the GET method to retrieve resources from the website.\n",
      "\n",
      "This tells us that the traffic is predominantly based on retrieving information, likely for browsing and searching purposes. The GET method is typically used for retrieval of data or HTML pages, whereas other HTTP methods like POST, PUT, and DELETE are used for updating or modifying server-side data.\n",
      "\n",
      "The prevalence of the GET method in these logs suggests that users are primarily interacting with the website to retrieve information, browse products, or access specific pages. This could be indicative of a typical web browsing behavior, where users visit the site to gather information, rather than actively submitting forms or performing complex actions.\n",
      "\n",
      "In conclusion, the analysis reveals that the HTTP GET method is predominantly used in the logs, indicating that the traffic is primarily focused on retrieving information from the website.\n",
      "\n",
      "Source Documents:\n",
      "1. GET /product/31349/61292/%D8%B3%D8%A7%D8%B9%D8%AA-%D9%85%DA%86%DB%8C-%D8%B9%D9%82%D8%B1%D8%A8%D9%87-%D8%A7%DB%8C-%D8%B2%D9%86%D8%A7%D9%86%D9%87-TISSOT-%D9%85%D8%AF%D9%84-T085.210.22.013.00 (Status: 20...\n",
      "2. GET /m/filter/b2,p3 (Status: 200, Size: 19538, IP: 83.121.230.255) (Datetime: 2019-01-24 20:35:51+03:30, User Agent: Mozilla/5.0 (Linux; Android 5.1.1; SM-J320F) AppleWebKit/537.36 (KHTML, like Gecko)...\n",
      "3. GET /filter/b211%2Cb502%2Cb214%2Cb389%2Cb335%2Cb554%2Cb265%2Cb549%2Cb142%2Cb487%2Cb153%2Cb579%2Cb428%2Cb412%2Cb353%2Cb307%2Cb742%2Cb95 (Status: 200, Size: 37587, IP: 17.58.102.43) (Datetime: 2019-01-2...\n",
      "4. GET /browse/home-appliances/%D9%84%D9%88%D8%A7%D8%B2%D9%85-%D8%AE%D8%A7%D9%86%DA%AF%DB%8C (Status: 499, Size: 0, IP: 5.113.215.131) (Datetime: 2019-01-23 22:41:16+03:30, User Agent: Mozilla/5.0 (Windo...\n",
      "5. GET /site/enamad (Status: 200, Size: 278, IP: 85.185.254.50) (Datetime: 2019-01-24 15:44:45+03:30, User Agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 S...\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"Are there any unusual patterns in user-agent strings that might indicate bot activity or potential attackers?\")\n",
    "ask_question(\"Which HTTP methods are predominantly used in the logs, and what does this tell us about the nature of the traffic?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
