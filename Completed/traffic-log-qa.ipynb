{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.2 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: numpy==1.26.4 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Collecting streamlit==1.1.0 (from -r requirements.txt (line 3))\n",
      "  Using cached streamlit-1.1.0-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: torch==2.0.1 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from -r requirements.txt (line 4)) (2.0.1)\n",
      "Requirement already satisfied: transformers>=4.30.2 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from -r requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: langchain==0.2.12 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from -r requirements.txt (line 6)) (0.2.12)\n",
      "Requirement already satisfied: langchain-huggingface>=0.0.3 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from -r requirements.txt (line 7)) (0.0.3)\n",
      "Requirement already satisfied: langchain-ollama==0.1.1 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from -r requirements.txt (line 8)) (0.1.1)\n",
      "Requirement already satisfied: langchain-community in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from -r requirements.txt (line 9)) (0.2.11)\n",
      "Requirement already satisfied: tqdm==4.65.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from -r requirements.txt (line 10)) (4.65.0)\n",
      "Requirement already satisfied: faiss-cpu in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from -r requirements.txt (line 11)) (1.8.0.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: altair>=3.2.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (5.4.0)\n",
      "Requirement already satisfied: astor in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (0.8.1)\n",
      "Requirement already satisfied: attrs in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (24.2.0)\n",
      "Requirement already satisfied: base58 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: blinker in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (1.8.2)\n",
      "Requirement already satisfied: cachetools>=4.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (5.5.0)\n",
      "Requirement already satisfied: click<8.0,>=7.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (7.1.2)\n",
      "Requirement already satisfied: packaging in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (10.4.0)\n",
      "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (3.20.3)\n",
      "Requirement already satisfied: pyarrow in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (17.0.0)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (0.9.1)\n",
      "Requirement already satisfied: requests in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: toml in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (0.10.2)\n",
      "Requirement already satisfied: tornado>=5.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (6.2)\n",
      "Requirement already satisfied: tzlocal in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (5.2)\n",
      "Requirement already satisfied: validators in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (0.33.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (3.1.43)\n",
      "Requirement already satisfied: watchdog in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from streamlit==1.1.0->-r requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: filelock in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: sympy in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 4)) (1.13.2)\n",
      "Requirement already satisfied: networkx in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (0.2.34)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (0.1.101)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain==0.2.12->-r requirements.txt (line 6)) (8.5.0)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain-ollama==0.1.1->-r requirements.txt (line 8)) (0.3.1)\n",
      "Requirement already satisfied: colorama in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from tqdm==4.65.0->-r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from transformers>=4.30.2->-r requirements.txt (line 5)) (0.24.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from transformers>=4.30.2->-r requirements.txt (line 5)) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from transformers>=4.30.2->-r requirements.txt (line 5)) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from transformers>=4.30.2->-r requirements.txt (line 5)) (0.19.1)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain-huggingface>=0.0.3->-r requirements.txt (line 7)) (3.0.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain-community->-r requirements.txt (line 9)) (0.6.7)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12->-r requirements.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12->-r requirements.txt (line 6)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12->-r requirements.txt (line 6)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12->-r requirements.txt (line 6)) (1.9.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 3)) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.1.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 9)) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from gitpython!=3.1.19->streamlit==1.1.0->-r requirements.txt (line 3)) (4.0.11)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.30.2->-r requirements.txt (line 5)) (2024.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain==0.2.12->-r requirements.txt (line 6)) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.12->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.12->-r requirements.txt (line 6)) (2.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 4)) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from requests->streamlit==1.1.0->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from requests->streamlit==1.1.0->-r requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from requests->streamlit==1.1.0->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from requests->streamlit==1.1.0->-r requirements.txt (line 3)) (2024.7.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface>=0.0.3->-r requirements.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: scipy in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface>=0.0.3->-r requirements.txt (line 7)) (1.14.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.12->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==1.1.0->-r requirements.txt (line 3)) (5.0.1)\n",
      "Requirement already satisfied: anyio in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (1.0.5)\n",
      "Requirement already satisfied: sniffio in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12->-r requirements.txt (line 6)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain==0.2.12->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 3)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 3)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 3)) (0.20.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface>=0.0.3->-r requirements.txt (line 7)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\ai\\anaconda\\envs\\myproject2\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface>=0.0.3->-r requirements.txt (line 7)) (3.5.0)\n",
      "Using cached streamlit-1.1.0-py2.py3-none-any.whl (8.3 MB)\n",
      "Installing collected packages: streamlit\n",
      "  Attempting uninstall: streamlit\n",
      "    Found existing installation: streamlit 1.37.1\n",
      "    Uninstalling streamlit-1.37.1:\n",
      "      Successfully uninstalled streamlit-1.37.1\n",
      "Successfully installed streamlit-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Web Traffic Log-Based Q&A System\n",
    "# \n",
    "# This project uses web traffic logs to create a question-answering system powered by a Retrieval-Augmented Generation (RAG) model. The system processes logs, stores them in a vector database, and uses two different language models (LLaMA 3 and Google T5) to generate answers to user queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
     }
    ],
   "source": [
    "import re\n",
    "\"\"\"\n",
    "Import necessary libraries and modules for the Q&A BOT ]]notebook.\n",
    "This code imports the following libraries and modules:\n",
    "- re: Regular expression operations.\n",
    "- pandas: Data manipulation and analysis.\n",
    "- numpy: Numerical computing.\n",
    "- datetime: Date and time manipulation.\n",
    "- faiss: Efficient similarity search and clustering of dense vectors.\n",
    "- tqdm: Progress bar for loops and tasks.\n",
    "- langchain.vectorstores: Vector stores for language embeddings.\n",
    "- langchain_huggingface: Hugging Face embeddings for language models.\n",
    "- langchain.docstore: Document store for storing and retrieving documents.\n",
    "- langchain.schema: Schema for defining document structure.\n",
    "- transformers: State-of-the-art natural language processing models.\n",
    "- langchain_huggingface: Hugging Face pipeline for language models.\n",
    "- langchain.chains: Retrieval-based question answering model.\n",
    "The code also checks for GPU availability, clears the GPU cache, and sets the device to either \"cuda\" or \"cpu\" based on availability.\n",
    "Note: The code assumes that the necessary libraries and modules are already installed.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import faiss\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.schema import Document\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Check for GPU availability\n",
    "import torch\n",
    "# Clear GPU cache before and after running the model\n",
    "torch.cuda.empty_cache()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log parsing and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import CSVLoader\n",
    "import re\n",
    "# The following code handles the loading, parsing, and preprocessing of web traffic logs. The logs are parsed to extract useful fields, and the data is then cleaned and structured for further use.\n",
    "\n",
    "# Function to parse the page content\n",
    "def parse_page_content(page_content):\n",
    "    \"\"\"\n",
    "    Parses the content of a web traffic log entry.\n",
    "    \n",
    "    Args:\n",
    "        page_content (str): A single log entry as a string.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing parsed fields like IP, identity, user, datetime, method, etc.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r'IP: (?P<ip>[\\d\\.]+)\\n'\n",
    "        r'Identity: (?P<identity>.+)\\n'\n",
    "        r'User: (?P<user>.+)\\n'\n",
    "        r'Timestamp: (?P<datetime>.+)\\n'\n",
    "        r'Request: (?P<method>\\w+) (?P<url>.+) HTTP/\\d\\.\\d\\n'\n",
    "        r'Status: (?P<status>\\d+)\\n'\n",
    "        r'Size: (?P<size>\\d+)\\n'\n",
    "        r'Referer: (?P<referer>.+)\\n'\n",
    "        r'User-Agent: (?P<user_agent>.+)'\n",
    "    )\n",
    "    match = pattern.search(page_content)\n",
    "    if match:\n",
    "        return match.groupdict()\n",
    "    return {}\n",
    "\n",
    "# Function to downsample the CSV file\n",
    "def downsample_csv(csv_file_path, sample_size, output_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    if sample_size and sample_size < len(df):\n",
    "        df = df.sample(n=sample_size)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    return output_file_path\n",
    "\n",
    "# Load and preprocess logs using CSVLoader\n",
    "def load_and_preprocess_logs(csv_file_path, sample_size=10000):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses web traffic logs from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path (str): Path to the CSV file containing log data.\n",
    "        sample_size (int): Number of samples to load from the file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing processed log data with additional fields for analysis.\n",
    "    \"\"\"\n",
    "    # Downsample the CSV file\n",
    "    downsampled_csv = downsample_csv(csv_file_path, sample_size, 'downsampled_logs.csv')\n",
    "    \n",
    "    # Load the downsampled CSV file\n",
    "    loader = CSVLoader(file_path=downsampled_csv)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Debug: Print the first few documents to check if they are loaded correctly\n",
    "    print(\"Loaded documents:\", documents[:5])\n",
    "    \n",
    "    # Convert documents to DataFrame\n",
    "    data = []\n",
    "    for doc in documents:\n",
    "        parsed_data = parse_page_content(doc.page_content)\n",
    "        data.append(parsed_data)\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Debug: Print the DataFrame to check if it is populated correctly\n",
    "    print(\"DataFrame head:\", df.head())\n",
    "    \n",
    "    # Preprocess logs\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format='%d/%b/%Y:%H:%M:%S %z', errors='coerce')\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['weekday'] = df['datetime'].dt.weekday\n",
    "    df['status'] = df['status'].astype(int, errors='ignore')\n",
    "    df['size'] = df['size'].astype(int, errors='ignore')\n",
    "    df['status_category'] = df['status'] // 100\n",
    "    \n",
    "    # Create a text field for embedding\n",
    "    df['text'] = df.apply(lambda row: f\"{row['method']} {row['url']} (Status: {row['status']}, Size: {row['size']}, IP: {row['ip']})\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "processed_logs = load_and_preprocess_logs('../processed_logs_sample.csv', sample_size=10000)\n",
    "\n",
    "print(processed_logs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# The following section creates a vector database using FAISS and stores the processed log data for efficient retrieval during query processing.\n",
    "\n",
    "\"\"\"\n",
    "Initialize the embedding model using HuggingFaceEmbeddings.\n",
    "Parameters:\n",
    "- model_name (str): The name of the Hugging Face model to use for embeddings.\n",
    "Returns:\n",
    "- embeddings (HuggingFaceEmbeddings): The initialized HuggingFaceEmbeddings object.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Create documents for vector store.\n",
    "Parameters:\n",
    "- processed_logs (DataFrame): The processed logs containing 'text', 'datetime', and 'user_agent' columns.\n",
    "Returns:\n",
    "- documents (list): The list of documents for the vector store, with additional information appended.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Split documents into smaller chunks.\n",
    "Parameters:\n",
    "- chunk_size (int): The size of each chunk.\n",
    "- chunk_overlap (int): The overlap between consecutive chunks.\n",
    "Returns:\n",
    "- texts (list): The list of split documents.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Create the vector store using FAISS.\n",
    "Parameters:\n",
    "- texts (list): The list of split documents.\n",
    "- embeddings (HuggingFaceEmbeddings): The initialized HuggingFaceEmbeddings object.\n",
    "Returns:\n",
    "- vectorstore (FAISS): The created FAISS vector store.\n",
    "\"\"\"\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create documents for vector store\n",
    "documents = [\n",
    "    f\"{row['text']} (Datetime: {row['datetime']}, User Agent: {row['user_agent']})\"\n",
    "    for _, row in processed_logs.iterrows()\n",
    "]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.create_documents(documents)\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rag Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# The RAG (Retrieval-Augmented Generation) model combines retrieval from the vector store and language generation using the LLaMA 3 model. It is set up here to handle user queries.\n",
    "\n",
    "\n",
    "\"\"\"\"\"\n",
    "This code initializes a retrieval-based question answering (QA) system for analyzing web traffic log entries. The system uses the OllamaLLM language model and the RetrievalQA chain from the langchain library.\n",
    "The main steps in the code are as follows:\n",
    "1. Set up logging for the system.\n",
    "2. Initialize the OllamaLLM language model with specific parameters.\n",
    "3. Define a template for generating prompts for the QA system.\n",
    "4. Create a PromptTemplate object with the defined template and input variables.\n",
    "5. Create a RetrievalQA chain using the OllamaLLM model, a retriever, and the PromptTemplate.\n",
    "6. The RetrievalQA chain is ready to be used for answering questions based on the provided log entries.\n",
    ".\"\"\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize OllamaLLM\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3\",  # or the specific model you're using\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "template = \"\"\"\"\n",
    "    You are an expert cybersecurity analyst specializing in web traffic log analysis. You have access to detailed log entries from a high-traffic website. Your task is to provide an in-depth analysis of these logs to answer specific questions posed by users.\n",
    "\n",
    "    When analyzing the logs, follow these guidelines:\n",
    "    1. **Data Integrity:** Ensure the accuracy and completeness of the data by cross-referencing multiple log entries where applicable.\n",
    "    2. **Pattern Recognition:** Identify and explain any significant patterns or anomalies in user behavior, such as:\n",
    "       - Repeated access from specific IP addresses.\n",
    "       - Unusual patterns in user-agent strings (indicating bots, crawlers, or potential attackers).\n",
    "       - Consistent access to specific pages or endpoints at unusual times.\n",
    "    3. **Contextual Correlation:** Correlate log entries across different dimensions (e.g., IP, time, URL) to build a coherent narrative of the events.\n",
    "    4. **Security Implications:** Assess and highlight any potential security concerns, such as:\n",
    "       - Signs of DDoS attacks.\n",
    "       - Unusual traffic spikes that could indicate brute force attempts.\n",
    "       - Access patterns that suggest vulnerability scanning.\n",
    "    5. **Detailed Justification:** For each observation or conclusion, provide specific log entries as evidence. Explain how these logs lead to your conclusions.\n",
    "\n",
    "    If the information requested is not available in the logs or if you are unable to determine an answer, clearly state that the data is inconclusive.\n",
    "\n",
    "    Below are the relevant log entries:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Based on the above logs, answer the following question with detailed reasoning and examples:\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# The following function allows users to input questions about the web traffic logs. The RAG model then processes the query and provides answers, along with relevant log entries as evidence.\n",
    "\n",
    "def ask_question(question):\n",
    "    \"\"\"\n",
    "    Processes a user query using the RAG model and returns an answer with relevant source documents.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question about the web traffic logs.\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints the question, the generated answer, and source documents.\n",
    "    \"\"\"\n",
    "    result = rag_chain({\"query\": question})\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "    print(\"\\nSource Documents:\")\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"{i}. {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"Are there any unusual patterns in user-agent strings that might indicate bot activity or potential attackers?\")\n",
    "ask_question(\"Which HTTP methods are predominantly used in the logs, and what does this tell us about the nature of the traffic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
